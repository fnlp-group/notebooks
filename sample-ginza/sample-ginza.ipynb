{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 参考資料\n",
    "GiNZAで始める日本語依存構造解析 〜CaboCha, UDPipe, Stanford NLPとの比較〜  \n",
    "https://www.slideshare.net/MegagonLabs/ginza-cabocha-udpipe-stanford-nlp\n",
    "\n",
    "言語処理学会第24回年次大会ワークショップ   \n",
    "形態素解析の今とこれから  \n",
    "https://sites.google.com/view/nlp2018ws/\n",
    "\n",
    "UD係り受けタグ一覧  \n",
    "https://qiita.com/kei_0324/items/400f639b2f185b39a0cf\n",
    "\n",
    "spaCy/GiNZA を用いた自然言語処理  \n",
    "https://www.ogis-ri.co.jp/otc/hiroba/technical/similar-document-search/part4.html\n",
    "\n",
    "janome  \n",
    "https://mocobeta.github.io/janome/\n",
    "\n",
    "Sentencepiece : ニューラル言語処理向けトークナイザ  \n",
    "https://qiita.com/taku910/items/7e52f1e58d0ea6e7859c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 要約\n",
    "* pip install xxxxの一行で始められる形態素解析\n",
    "* 2019年4月にようやくUniversal Dependenciesの日本語対応ライブラリが登場\n",
    "* **spaCy(GiNZA)**, **SudachiPy**, **janome**, **SentencePiece**の4つの形態素解析器の紹介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy(GiNZA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 概要\n",
    "mecab, juman, chasenなどと同じ形態素解析器。UniDicベース。  \n",
    "Universal Dependencies（UD）に準拠。\n",
    "Sudachiという形態素解析器をベースに、spaCyで利用可能な日本語言語モデルを提供。\n",
    "\n",
    ">リクルートと国立国語研究所から、Universal Dependencyに基づいた係り受け解析が可能なライブラリGiNZAが公開されました！今までcabochaでは関係がわからない、KNPはちょっと遅い、といろいろ問題があったところですが、spaCyベースという共通＋モダンな形で解析が可能になりました。\n",
    "*（twitter）*\n",
    "\n",
    "### 特徴\n",
    "* Universal Dependenciesのルールで文の構造を解析。\n",
    "* 固有表現抽出（NER）や句構造の抽出が可能\n",
    "* 単語ベクトルをモデルに内包できる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"https://github.com/megagonlabs/ginza/releases/download/latest/ginza-latest.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('ja_ginza')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = '''モバイル決済サービスを手掛けるスクエア（@SQ/U）株が冴えない。25日の終値は72.65ドルと相場上昇に歩調を合わせて回復しているものの、昨年10月につけた上場来高値（101.15ドル）の７割程度にとどまる。2019年１～３月期決算で赤字幅が拡大することが上値を重くしている。決算発表は５月１日の大引け後（日本時間28日）に予定されている。\n",
    "\n",
    "▼市場予想 \n",
    "                19年１～３月期　　　　　 \n",
    "売上高　　　　　４億8000万ドル（56％増）\n",
    "純利益　　　　  4300万ドルの赤字(2400万ドルの赤字)\n",
    "ＥＰＳ　 　　　 0.08（0.06ドルの赤字）　　　    \n",
    "※QUICK FactSet Workstationの４月25日時点のデータを使用。（）内は前年同期比。\n",
    "\n",
    "▼赤字体質への懸念で株価は市場平均を下回る\n",
    "　2018年10～12月期の純損益は2800万円の赤字と、前年同期（1600万円の赤字）から赤字幅が拡大した。19年１～３月期決も赤字が続く見通し。将来の成長に向けた研究費や販管費の増加が収益を圧迫する見込みだが、赤字体質の定着が懸念される。市場予想によると、１～３月期は決済サービスや中小企業向け少額貸し付サービスが堅調だったようだ。決済取扱高は230億ドルと３割近く増えたもよう。\n",
    "\n",
    "　年初からの株価をみてもスクエア株はＳ＆Ｐ500種株価指数やダウ工業株30種平均を下回って推移している。ただ、野村インスティネットは３月末のリポートで「18年10～12月期決算で大型加盟店の決済処理額の伸び率が鈍化したことは懸念材料。しかし投資家は大局観を見失っている。マーケティングの効果や調整後の手数料率といった、カギとなるデータを見過ごしている」と指摘。投資判断は「ＢＵＹ」、目標株価を105ドルとした。\n",
    "　一方、ウェドブッシュ証券はバリュエーションを総合的に判断したうえで、目標株価を75ドルとしており、現水準程度が妥当とみている。\n",
    "'''\n",
    "text1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 形態素解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc = nlp(text1)\n",
    "for sent in doc.sents:\n",
    "    for token in sent:\n",
    "        print('{:<5}{:<10}{:<10}{:<10}{:<20}{:<10}{:<10}{:<5}{}'.format(\n",
    "            token.i,          # 単語の位置\n",
    "            token.orth_,      # 単語\n",
    "            token.lemma_,     # 単語の原型\n",
    "            token.pos_,       # 品詞\n",
    "            token.tag_,       # 品詞分類\n",
    "            token.dep_,       # 係り受け関係\n",
    "            token.ent_type_,  # 固有表現\n",
    "            token._.bunsetu_bi_label, # 文節\n",
    "            token.head.i      # 係り受け先の単語の位置\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"dep\", options={\"compact\":True, \"bg\": \"#09a3d5\", \"color\":\"white\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 固有表現抽出\n",
    "いわゆるタグ付け。\n",
    "\n",
    "固有表現抽出では**Flair**というライブラリが有名。  \n",
    "github：https://github.com/zalandoresearch/flair  \n",
    "日本語の情報：https://hironsan.hatenablog.com/entry/implementing-contextual-string-embeddings-for-named-entity-recognition?utm_campaign=piqcy&utm_medium=email&utm_source=Revue%20newsletter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print('{:<30}{}'.format(ent.text, ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 名詞句の抽出\n",
    "未知語の抽出などで使えるかも（ngramの共起ベースで未知語を抽出する場合、単純な分かち書きの結果にgensimのPhrasesというクラスで共起スコアを計算できます。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for np in doc.noun_chunks:\n",
    "    print(np, np.ent_id_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 関係認識(Entity relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_spans(spans):\n",
    "    # 文字列の長さ（より長いもの）、文中の位置（より後ろのもの）でソート\n",
    "    get_sort_key = lambda span: (span.end - span.start, span.start)\n",
    "    sorted_spans = sorted(spans, key=get_sort_key, reverse=True)\n",
    "    result = []\n",
    "    seen_tokens = set()\n",
    "    for span in sorted_spans:\n",
    "        if span.start not in seen_tokens and span.end - 1 not in seen_tokens:\n",
    "            result.append(span)\n",
    "            seen_tokens.update(range(span.start, span.end))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ = '当期の経営成績を分析しますと、次のとおりであります。収益は、石炭などの資源価格の上昇や取扱数量増加による金属・資源での増収や、国内外自動車ディーラー事業などの新規取得による自動車での増収などにより、1兆8,561億90百万円と前期比2.19％の増収となりました。売上総利益は、収益の増加などにより、前期比85億76百万円増加の2,409億56百万円となりました。\\u3000税引前利益は、売上総利益の増益に加え、LNG事業会社の増益などによる持分法による投資損益の増益などにより、前期比145億39百万円増加の948億82百万円となりました。\\u3000当期純利益は、税引前利益948億82百万円から、法人所得税費用196億62百万円を控除した結果、前期比135億25百万円増加の752億19百万円となりました。また、親会社の所有者に帰属する当期純利益（以下、当期純利益という）は前期比135億77百万円増加し、704億19百万円となりました。\\u3000在外営業活動体の換算差額やその他の包括利益を通じて公正価値で測定する金融資産の減少がありましたが、当期純利益の増益などにより、当期包括利益は前期比36億22百万円増加し、549億48百万円となりました。また、親会社の所有者に帰属する当期包括利益は前期比35億8百万円増加し、509億38百万円となりました。次に、これをセグメント別に分析しますと、以下のとおりであります。'\n",
    "text_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "doc1 = nlp(text_)\n",
    "[t for sent in doc1.sents for t in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc1, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spans = list(doc1.ents) + list(doc1.noun_chunks)\n",
    "spans = filter_spans(spans)\n",
    "\n",
    "# 単語の区切りを、固有表現や名詞句の塊で置き換える\n",
    "with doc1.retokenize() as retokenizer:\n",
    "    for span in spans:\n",
    "        retokenizer.merge(span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "[t for sent in doc1.sents for t in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nmod : 名詞修飾語\n",
    "# nsumg : 主語名詞\n",
    "for money in filter(lambda w: w.ent_type_ == \"MONEY\", doc1):\n",
    "    if money.dep_ in (\"nmod\"):\n",
    "        subject = [w for w in money.head.lefts if w.dep_ == \"nsubj\"]\n",
    "        if subject:\n",
    "            subject = subject[0]\n",
    "            print(\"{:<10}\\t--->\\t{}\\t--->\\t{}\".format(subject.text, money.ent_type_, money.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc1, style=\"dep\", options={\"compact\":True, \"bg\": \"#09a3d5\", \"color\":\"white\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text_with_ws\n",
    "空白文字の情報を保持"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text3 = '今回のコラボレーションは、2019年9月14日（土）に表参道・青山・原宿エリアで開催される「VOGUE FASHION’S NIGHT OUT2019」に参加する「The SHEL’TTER TOKYO」とコラボレーションを記念して実現しました。'\n",
    "text3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "d = nlp(text3)\n",
    "for sent in d.sents:\n",
    "    for token in sent:\n",
    "        print(token.orth_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(''.join([t.text_with_ws for s in d.sents for t in s]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文書・文・単語などのベクトル化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in doc.sents:\n",
    "    print(sent.similarity(doc))\n",
    "    print(sent)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print('{:.5f}  {}'.format(ent.similarity(doc), ent.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# SudachiPy\n",
    "GiNZAのベースになっているライブラリ。  \n",
    "Sudachi(Java)はElasticSearchと組合わせて利用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sudachipy import tokenizer\n",
    "from sudachipy import dictionary\n",
    "\n",
    "tokenizer_obj = dictionary.Dictionary().create()\n",
    "mode = tokenizer.Tokenizer.SplitMode.C\n",
    "\n",
    "for i in tokenizer_obj.tokenize(text1, mode):\n",
    "    print(i.surface(), i.part_of_speech(), i.normalized_form())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# janome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 概要\n",
    "依存ライブラリなしで書かれた辞書内包の形態素解析器です。  \n",
    "辞書としてmecab-ipadic-2.7.0-20070801を内包（mecabとだいたい同じ）。\n",
    "\n",
    "### 特徴\n",
    "* 依存ライブラリ無し、pythonのみで実装されてるので、pythonが動けば確実に動きます。\n",
    "* 実行速度が遅い。\n",
    "* 特にこだわりが無ければ、一番手軽に使える形態素解析器です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install janome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "t = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in t.tokenize(text1):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分かち書きモード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.tokenize(text1, wakati=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 複合名詞の抽出（filter）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from janome.analyzer import Analyzer\n",
    "from janome.tokenfilter import *\n",
    "\n",
    "token_filters = [CompoundNounFilter()]\n",
    "a = Analyzer(token_filters=token_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in a.analyze(text1):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# SentencePiece\n",
    "### 概要\n",
    "DeepLearning向けの（機械のための）形態素解析。  \n",
    "End to Endで利用することを前提に、効率よく文字列を分割する。\n",
    "\n",
    "### 特徴\n",
    "* subwordベースでの分かち書き\n",
    "* 語彙数を少なくできる（メモリ節約）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 手元のデータでモデルを学習させる場合\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    \"--input=train_text.txt --model_prefix=trained_model --vocab_size=16000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 日本語の学習済みモデル(wikipedia)\n",
    "# https://drive.google.com/drive/folders/1Zsm9DD40lrUVu6iAnIuTH2ODIkh-WM-O\n",
    "# wiki-ja.model\n",
    "\n",
    "sp.Load('wiki-ja.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text4 = '個人向け資産運用コンサルティング営業の業務最適化を実現し、金融商品購買プロセスにおけるお客様満足度の向上を支援するソリューションを提供します。'\n",
    "text4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "splitted_list = sp.EncodeAsPieces(text4)\n",
    "splitted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = nlp(text4)\n",
    "for sent in d.sents:\n",
    "    for token in sent:\n",
    "        print(token.orth_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# まとめ\n",
    "\n",
    "* spaCy(GiNZA) : 今後に期待  \n",
    "\n",
    "* janome : とりあえず分かち書きしたいときに  \n",
    "\n",
    "* SentencePiece : DeepLearningで使えるかも"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
